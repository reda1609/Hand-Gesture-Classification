{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afbe5b47",
   "metadata": {},
   "source": [
    "# Hand Gesture Classification Using MediaPipe & HaGRID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59887cea",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10728f",
   "metadata": {},
   "source": [
    "This project focuses on the classification of hand gestures using landmark data generated by **MediaPipe** from the **HaGRID (Hand Gesture Recognition Image Dataset)**. By utilizing the spatial coordinates of hand keypoints, we aim to train a machine learning model capable of accurately identifying various gestures. For experiment tracking and reproducibility, we will leverage **MLflow** to log parameters, metrics, and artifacts throughout the model development process. This will allow us to systematically evaluate different models and configurations, ultimately selecting the best-performing model for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e12616",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import mediapipe as mp\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, ParameterGrid, train_test_split\n",
    "\n",
    "import mlflow\n",
    "from mlflow.data.pandas_dataset import from_pandas\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Hand Gesture Classification\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55ba56",
   "metadata": {},
   "source": [
    "## Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bae048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data\\hand_landmarks_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c634e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,6))\n",
    "sns.countplot(data=df, x='label', hue='label')\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf5c20",
   "metadata": {},
   "source": [
    "So far we don't have any missing values or inconsistency in data types, our data contains 25675 rows and the data is quite unbalanced (we have shortage in fist & mute labels especially)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce923f",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ba530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hand_landmarks(sample, ax=None):\n",
    "    \"\"\"\n",
    "    Plot hand landmarks for a single sample.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample : pd.Series or dict\n",
    "        A row from the dataframe containing x, y, z coordinates for 21 landmarks\n",
    "    ax : matplotlib axis, optional\n",
    "        Axis to plot on. If None, creates a new figure\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ax : matplotlib axis\n",
    "        The axis with the plotted hand landmarks\n",
    "    \"\"\"\n",
    "    # Extract x, y coordinates for all 21 landmarks\n",
    "    x_coords = [sample[f'x{i}'] for i in range(1, 22)]\n",
    "    y_coords = [sample[f'y{i}'] for i in range(1, 22)]\n",
    "    \n",
    "    # Create figure if not provided\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    \n",
    "    # MediaPipe hand landmark connections\n",
    "    # Each tuple represents a connection between two landmark indices\n",
    "    connections = [\n",
    "        # Thumb\n",
    "        (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "        # Index finger\n",
    "        (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "        # Middle finger\n",
    "        (0, 9), (9, 10), (10, 11), (11, 12),\n",
    "        # Ring finger\n",
    "        (0, 13), (13, 14), (14, 15), (15, 16),\n",
    "        # Pinky\n",
    "        (0, 17), (17, 18), (18, 19), (19, 20),\n",
    "        # Palm\n",
    "        (5, 9), (9, 13), (13, 17)\n",
    "    ]\n",
    "    \n",
    "    # Plot connections (lines between landmarks)\n",
    "    for connection in connections:\n",
    "        start_idx, end_idx = connection\n",
    "        ax.plot([x_coords[start_idx], x_coords[end_idx]], \n",
    "                [y_coords[start_idx], y_coords[end_idx]], \n",
    "                'b-', linewidth=2, alpha=0.6)\n",
    "    \n",
    "    # Plot landmarks (points)\n",
    "    ax.scatter(x_coords, y_coords, c='red', s=50, zorder=3)\n",
    "    \n",
    "    # Add landmark numbers\n",
    "    for i, (x, y) in enumerate(zip(x_coords, y_coords)):\n",
    "        ax.annotate(str(i), (x, y), fontsize=8, ha='center', \n",
    "                   bbox=dict(boxstyle='circle,pad=0.1', facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    # Invert y-axis (image coordinates start from top-left)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('X Coordinate', fontsize=12)\n",
    "    ax.set_ylabel('Y Coordinate', fontsize=12)\n",
    "    ax.set_title(f'Hand Gesture: {sample[\"label\"]}', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_gestures(df, n_samples=6, random_state=42):\n",
    "    \"\"\"\n",
    "    Plot multiple hand gesture samples for visual inspection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing hand landmark data\n",
    "    n_samples : int\n",
    "        Number of samples to plot (default: 6)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    # Sample random gestures\n",
    "    samples = df.sample(n=n_samples, random_state=random_state)\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_cols = 3\n",
    "    n_rows = (n_samples + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    axes = axes.flatten() if n_samples > 1 else [axes]\n",
    "    \n",
    "    # Plot each sample\n",
    "    for idx, (_, sample) in enumerate(samples.iterrows()):\n",
    "        plot_hand_landmarks(sample, ax=axes[idx])\n",
    "    \n",
    "    # Hide extra subplots if any\n",
    "    for idx in range(n_samples, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gestures_by_label(df, labels=None, samples_per_label=3, random_state=42):\n",
    "    \"\"\"\n",
    "    Plot hand gesture samples grouped by label for comparison.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing hand landmark data\n",
    "    labels : list, optional\n",
    "        List of gesture labels to plot. If None, plots all unique labels\n",
    "    samples_per_label : int\n",
    "        Number of samples to show per label (default: 3)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    # Get labels to plot\n",
    "    if labels is None:\n",
    "        labels = df['label'].unique()[:5]  # Limit to 5 labels for readability\n",
    "    \n",
    "    n_labels = len(labels)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_labels, samples_per_label, \n",
    "                             figsize=(5 * samples_per_label, 5 * n_labels))\n",
    "    \n",
    "    # Handle single row/column case\n",
    "    if n_labels == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif samples_per_label == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Plot samples for each label\n",
    "    for row_idx, label in enumerate(labels):\n",
    "        # Get samples for this label\n",
    "        label_samples = df[df['label'] == label].sample(\n",
    "            n=min(samples_per_label, len(df[df['label'] == label])), \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        for col_idx, (_, sample) in enumerate(label_samples.iterrows()):\n",
    "            if col_idx < samples_per_label:\n",
    "                plot_hand_landmarks(sample, ax=axes[row_idx, col_idx])\n",
    "    \n",
    "    plt.suptitle('Hand Gestures Grouped by Label', fontsize=16, fontweight='bold', y=1.001)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd957fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_gestures(df, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gestures_by_label(df, labels=['call', 'peace', 'fist'], samples_per_label=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd1128",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12d89a",
   "metadata": {},
   "source": [
    "Before applying any operations on the dataset we are going to split the data to 80% training and 20% validation and the testing will be done real time on the output video this way we are preventing any data leakage risks.\n",
    "\n",
    "Since we are having quite unbalanced data so it is prefered to use stratified sampling during splitting the data.\n",
    "\n",
    "Additionally, the data should be shuffled before splitting since consecutive samples belong to the same gesture class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b27cc",
   "metadata": {},
   "source": [
    "The detected hands have different scales and positions in the image. To overcome this problem recenter the hand landmarks (x,y) to make the origin the wrist point and divide all the landmarks by the mid-finger tip position.\n",
    "\n",
    "We are going to create a custom sklearn class that transfrom our data by subtracting the wrist coordinates from all other landmark coordinates and scale them by the position of the middle finger tip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8acf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkNormalizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to normalize MediaPipe hand landmarks.\n",
    "    - Recenters (x, y) coordinates to the wrist (Landmark 0).\n",
    "    - Scales landmarks by the distance to the middle finger tip (Landmark 12).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No parameters to estimate here\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = np.copy(X)\n",
    "        \n",
    "        for i in range(len(X_copy)):\n",
    "            # Reshape to (21, 3) for easier indexing: [landmark_idx, coordinate]\n",
    "            landmarks = X_copy[i].reshape(21, 3)\n",
    "            \n",
    "            # Recenter: Subtract wrist (0) coordinates from all (x, y) \n",
    "            wrist = landmarks[0, :2] # Only x and y\n",
    "            landmarks[:, :2] -= wrist\n",
    "            \n",
    "            # Scale: Divide by mid-finger tip (12) position\n",
    "            scale_factor = np.linalg.norm(landmarks[12, :2])\n",
    "            landmarks[:, :2] /= scale_factor\n",
    "                \n",
    "            # Flatten back to the original shape\n",
    "            X_copy[i] = landmarks.flatten()\n",
    "            \n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69bd4d",
   "metadata": {},
   "source": [
    "Now we are going to test this transformer and create a new dataframe using the normalized data to visualize the landmarks after normalization and make sure very thing is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f6d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = LandmarkNormalizer()\n",
    "X_train_normalized = normalizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = pd.DataFrame(data=np.column_stack([X_train_normalized, y_train]), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440433d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_gestures(df_normalized, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e32408",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gestures_by_label(df_normalized, labels=['call', 'peace', 'fist'], samples_per_label=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de456de8",
   "metadata": {},
   "source": [
    "As you see the wrist landmark is at zero and all other landmarks are correctly scaled without changing the aspect ratio of the hand this makes our model scale and translation invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a50b82",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9b0ef",
   "metadata": {},
   "source": [
    "In this section, we are going to train three different models using k-fold cross validation and grid search for hyperparameter tuning:\n",
    "\n",
    "1. **K-Nearest Neighbors (KNN)**: A distance-based classifier that predicts based on the majority class of the k nearest neighbors.\n",
    "\n",
    "2. **Logistic Regression**: A linear model that uses a logistic function to model the probability of each gesture class.\n",
    "\n",
    "3. **Random Forest**: An ensemble learning method that constructs multiple decision trees and outputs the mode of their predictions.\n",
    "\n",
    "For each model, we will:\n",
    "- Perform grid search to find the best hyperparameters\n",
    "- Use 5-fold cross-validation to evaluate performance during training\n",
    "- Test the best model on the hold-out test set\n",
    "- Visualize results using confusion matrices and classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf069d3",
   "metadata": {},
   "source": [
    "For tracking our experiments, we will utilize **MLflow** to log parameters, metrics, and artifacts for each run. This will allow us to compare different models and hyperparameter configurations effectively. In the end we will analyze the results and choose the best performing model depending on f1-score and we will register it in the **MLflow Model Registry** and move it to the \"production\" stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_normalized = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e54953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate datasets for training and testing to log correctly in MLflow\n",
    "df_train = pd.DataFrame(data=np.column_stack([X_train, y_train]), columns=df.columns)\n",
    "df_test = pd.DataFrame(data=np.column_stack([X_test, y_test]), columns=df.columns)\n",
    "\n",
    "train_dataset = from_pandas(df_train, source=\"data/hand_landmarks_data.csv\", name=\"hand_landmarks_train\")\n",
    "test_dataset = from_pandas(df_test, source=\"data/hand_landmarks_data.csv\", name=\"hand_landmarks_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70a6da",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49296f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "run_no = 1\n",
    "\n",
    "for param in ParameterGrid(param_grid):\n",
    "    with mlflow.start_run(run_name=f\"KNN_{run_no}\"):\n",
    "\n",
    "        knn = KNeighborsClassifier(**param)\n",
    "        scores = cross_val_score(knn, X_train_normalized, y_train, cv=5)\n",
    "\n",
    "        mean_score = np.mean(scores)\n",
    "\n",
    "        mlflow.log_params(param)\n",
    "        mlflow.log_metric(\"mean_cv_accuracy\", mean_score)\n",
    "        mlflow.log_input(train_dataset, context=\"training\")\n",
    "\n",
    "        run_no += 1\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(\"Hand Gesture Classification\")\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id], \n",
    "    order_by=[\"metrics.mean_cv_accuracy DESC\"], \n",
    "    filter_string=\"tags.mlflow.runName LIKE 'KNN_%'\"\n",
    ")\n",
    "\n",
    "best_run = runs.iloc[0]\n",
    "best_params = {\n",
    "    'n_neighbors': int(best_run['params.n_neighbors']),\n",
    "    'weights': best_run['params.weights'],\n",
    "    'metric': best_run['params.metric']\n",
    "}\n",
    "best_score = best_run['metrics.mean_cv_accuracy']\n",
    "\n",
    "print(f\"Best Params: {best_params}, Best CV Accuracy: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Best_KNN_Model_Evaluation\"):\n",
    "    knn = KNeighborsClassifier(**best_params)\n",
    "    best_model = knn.fit(X_train_normalized, y_train)\n",
    "    preds = best_model.predict(X_test_normalized)\n",
    "\n",
    "    report = classification_report(y_test, preds, output_dict=True)\n",
    "\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"accuracy\", report['accuracy'])\n",
    "    mlflow.log_metric(\"precision\", report['weighted avg']['precision'])\n",
    "    mlflow.log_metric(\"recall\", report['weighted avg']['recall'])\n",
    "    mlflow.log_metric(\"f1_score\", report['weighted avg']['f1-score'])\n",
    "    mlflow.log_input(train_dataset, context=\"training\")\n",
    "    mlflow.log_input(test_dataset, context=\"testing\")\n",
    "\n",
    "    print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {report['weighted avg']['precision']:.4f}\")\n",
    "    print(f\"Recall: {report['weighted avg']['recall']:.4f}\")\n",
    "    print(f\"F1-score: {report['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix - KNN')\n",
    "    plt.savefig(\"confusion_matrix_knn.png\")\n",
    "    plt.show()\n",
    "\n",
    "    mlflow.log_artifact(\"confusion_matrix_knn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12e439",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l2', None],\n",
    "    'max_iter': [500, 1000]\n",
    "}\n",
    "\n",
    "run_no = 1\n",
    "\n",
    "for param in ParameterGrid(param_grid):\n",
    "    with mlflow.start_run(run_name=f\"LogisticRegression_{run_no}\"):\n",
    "\n",
    "        lr = LogisticRegression(**param, random_state=42)\n",
    "        scores = cross_val_score(lr, X_train_normalized, y_train, cv=5)\n",
    "\n",
    "        mean_score = np.mean(scores)\n",
    "\n",
    "        mlflow.log_params(param)\n",
    "        mlflow.log_metric(\"mean_cv_accuracy\", mean_score)\n",
    "        mlflow.log_input(train_dataset, context=\"training\")\n",
    "\n",
    "        run_no += 1\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(\"Hand Gesture Classification\")\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id], \n",
    "    order_by=[\"metrics.mean_cv_accuracy DESC\"], \n",
    "    filter_string=\"tags.mlflow.runName LIKE 'LogisticRegression_%'\"\n",
    ")\n",
    "\n",
    "best_run = runs.iloc[0]\n",
    "best_params = {\n",
    "    'C': float(best_run['params.C']),\n",
    "    'penalty': best_run['params.penalty'] if best_run['params.penalty'] != 'None' else None,\n",
    "    'max_iter': int(best_run['params.max_iter'])\n",
    "}\n",
    "best_score = best_run['metrics.mean_cv_accuracy']\n",
    "\n",
    "print(f\"Best Params: {best_params}, Best CV Accuracy: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Best_LogisticRegression_Model_Evaluation\"):\n",
    "    lr = LogisticRegression(**best_params, random_state=42)\n",
    "    best_model = lr.fit(X_train_normalized, y_train)\n",
    "    preds = best_model.predict(X_test_normalized)\n",
    "\n",
    "    report = classification_report(y_test, preds, output_dict=True)\n",
    "\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"accuracy\", report['accuracy'])\n",
    "    mlflow.log_metric(\"precision\", report['weighted avg']['precision'])\n",
    "    mlflow.log_metric(\"recall\", report['weighted avg']['recall'])\n",
    "    mlflow.log_metric(\"f1_score\", report['weighted avg']['f1-score'])\n",
    "    mlflow.log_input(train_dataset, context=\"training\")\n",
    "    mlflow.log_input(test_dataset, context=\"testing\")\n",
    "\n",
    "    print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {report['weighted avg']['precision']:.4f}\")\n",
    "    print(f\"Recall: {report['weighted avg']['recall']:.4f}\")\n",
    "    print(f\"F1-score: {report['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix - Logistic Regression')\n",
    "    plt.savefig(\"confusion_matrix_lr.png\")\n",
    "    plt.show()\n",
    "\n",
    "    mlflow.log_artifact(\"confusion_matrix_lr.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a9043",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "run_no = 1\n",
    "\n",
    "for param in ParameterGrid(param_grid):\n",
    "    with mlflow.start_run(run_name=f\"RandomForest_{run_no}\"):\n",
    "\n",
    "        rf = RandomForestClassifier(**param, random_state=42)\n",
    "        scores = cross_val_score(rf, X_train_normalized, y_train, cv=5)\n",
    "\n",
    "        mean_score = np.mean(scores)\n",
    "\n",
    "        mlflow.log_params(param)\n",
    "        mlflow.log_metric(\"mean_cv_accuracy\", mean_score)\n",
    "        mlflow.log_input(train_dataset, context=\"training\")\n",
    "\n",
    "        run_no += 1\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(\"Hand Gesture Classification\")\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id], \n",
    "    order_by=[\"metrics.mean_cv_accuracy DESC\"], \n",
    "    filter_string=\"tags.mlflow.runName LIKE 'RandomForest_%'\"\n",
    ")\n",
    "\n",
    "best_run = runs.iloc[0]\n",
    "best_params = {\n",
    "    'n_estimators': int(best_run['params.n_estimators']),\n",
    "    'max_depth': int(best_run['params.max_depth']) if best_run['params.max_depth'] != 'None' else None,\n",
    "    'min_samples_split': int(best_run['params.min_samples_split'])\n",
    "}\n",
    "best_score = best_run['metrics.mean_cv_accuracy']\n",
    "\n",
    "print(f\"Best Params: {best_params}, Best CV Accuracy: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e835c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Best_RandomForest_Model_Evaluation\"):\n",
    "    best_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "    best_model.fit(X_train_normalized, y_train)\n",
    "    preds = best_model.predict(X_test_normalized)\n",
    "\n",
    "    report = classification_report(y_test, preds, output_dict=True)\n",
    "\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"accuracy\", report['accuracy'])\n",
    "    mlflow.log_metric(\"precision\", report['weighted avg']['precision'])\n",
    "    mlflow.log_metric(\"recall\", report['weighted avg']['recall'])\n",
    "    mlflow.log_metric(\"f1_score\", report['weighted avg']['f1-score'])\n",
    "    mlflow.log_input(train_dataset, context=\"training\")\n",
    "    mlflow.log_input(test_dataset, context=\"testing\")\n",
    "\n",
    "    print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {report['weighted avg']['precision']:.4f}\")\n",
    "    print(f\"Recall: {report['weighted avg']['recall']:.4f}\")\n",
    "    print(f\"F1-score: {report['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix - Random Forest')\n",
    "    plt.savefig(\"confusion_matrix_rf.png\")\n",
    "    plt.show()\n",
    "\n",
    "    mlflow.log_artifact(\"confusion_matrix_rf.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b4378",
   "metadata": {},
   "source": [
    "## Final Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fe39b",
   "metadata": {},
   "source": [
    "Now we are going to select the best performing model based on the evaluation metrics among all the models that were run in the experiment. The best model is determined by the highest F1-score achieved during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = mlflow.get_experiment_by_name(\"Hand Gesture Classification\")\n",
    "\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id], \n",
    "    order_by=[\"metrics.f1_score DESC\"], \n",
    "    filter_string=\"tags.mlflow.runName LIKE '%_Evaluation'\"\n",
    ")\n",
    "\n",
    "best_run = runs.iloc[0]\n",
    "best_model_name = best_run['tags.mlflow.runName'].split('_')[1]\n",
    "best_f1_score = best_run['metrics.f1_score']\n",
    "print(f\"Best Model: {best_model_name}, Best F1-score: {best_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212d44b",
   "metadata": {},
   "source": [
    "Based on the evaluation results, the **Random Forest Classifier** demonstrated the best performance. We'll now construct a complete pipeline that integrates the landmark normalization preprocessing step with the best random forest model, and train it on the entire dataset (combining training and test sets) to maximize the use of available data for the final production model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65828f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with the normalizer and the best random forest model\n",
    "final_pipeline = Pipeline([\n",
    "    ('normalizer', LandmarkNormalizer()),\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        min_samples_split=best_params['min_samples_split'],\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Combine training and test data for final training\n",
    "X_full = pd.concat([X_train, X_test], axis=0)\n",
    "y_full = pd.concat([y_train, y_test], axis=0)\n",
    "\n",
    "print(f\"Training final model on complete dataset: {X_full.shape[0]} samples\")\n",
    "\n",
    "# Train the pipeline on the full dataset\n",
    "final_pipeline.fit(X_full, y_full)\n",
    "\n",
    "print(\"\\nFinal model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41e0fc",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012319ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Extract hand landmarks from MediaPipe results and convert to flat array.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hand_landmarks : mediapipe hand landmarks\n",
    "        Hand landmarks from MediaPipe detection\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    landmarks_array : numpy array\n",
    "        Flattened array of 63 values (21 landmarks Ã— 3 coordinates)\n",
    "    \"\"\"\n",
    "    landmarks = []\n",
    "    for landmark in hand_landmarks.landmark:\n",
    "        landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "    return np.array(landmarks).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get video properties for saving\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS)) or 20  # Default to 20 if fps is 0\n",
    "\n",
    "# Create output filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_path = f\"output_videos/hand_gesture_recognition_{timestamp}.mp4\"\n",
    "\n",
    "# Initialize video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "print(f\"Video will be saved to: {output_path}\")\n",
    "\n",
    "# Initialize MediaPipe Hands with confidence thresholds\n",
    "with mp_hands.Hands(model_complexity=0, min_detection_confidence=0.5, min_tracking_confidence=0.5, max_num_hands=2) as hands:\n",
    "    print(\"Starting real-time hand gesture recognition...\")\n",
    "    print(\"Press 'q' to quit\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        # Flip the frame horizontally for a selfie-view display\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Convert the BGR image to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the frame with MediaPipe\n",
    "        results = hands.process(frame_rgb)\n",
    "        \n",
    "        # Draw hand landmarks and predict gestures\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                # Draw hand landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style()\n",
    "                )\n",
    "                \n",
    "                # Extract landmarks\n",
    "                landmarks_array = extract_landmarks(hand_landmarks)\n",
    "                \n",
    "                # Run prediction using the final pipeline\n",
    "                prediction = final_pipeline.predict(landmarks_array)[0]\n",
    "                \n",
    "                # Get hand label (Left/Right)\n",
    "                hand_label = results.multi_handedness[hand_idx].classification[0].label\n",
    "                \n",
    "                # Draw prediction text in the upper left corner\n",
    "                text = f\"{hand_label}: {prediction}\"\n",
    "                text_x = 10\n",
    "                text_y = 70 + (hand_idx * 40)  # Stack multiple hands vertically\n",
    "                \n",
    "                # Draw background rectangle for better text visibility\n",
    "                (text_width, text_height), _ = cv2.getTextSize(\n",
    "                    text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2\n",
    "                )\n",
    "                cv2.rectangle(frame, (text_x - 5, text_y - text_height - 5), (text_x + text_width + 5, text_y + 5), (0, 0, 0), -1)\n",
    "                \n",
    "                # Draw text\n",
    "                cv2.putText(frame,text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display instructions\n",
    "        cv2.putText(frame, \"Press 'q' to quit\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Write frame to output video\n",
    "        out.write(frame)\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow('Hand Gesture Recognition', frame)\n",
    "        \n",
    "        # Break on 'q' key press\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Video capture ended. Video saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
